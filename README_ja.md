# Quartzの駄目データの手引き

**現実世界のデータに見られる問題に対する、どう解決するかについての示唆とあわせた、網羅的な参照**

報告者たるとき、あなたの世界はデータで満ちている。そして、それらのデータは問題で一杯だ。この手引きは、データを使って作業する際に出くわすであろう種々の問題の多くに対する、徹底的な記述、および示唆される解決策を提示する。

これらの問題のほとんどは解決できる。一部は解決できず、そしてそのことは、そのデータを使うべきではない、ということを意味する。他のものは、解決できないのだが、予防策があればそのデータを使い続けられる。これらの不明確な状況を見越しておくために、本ガイドは、問題を解決するための技術を最もよく身につけている人——つまり、あなたや、あなたの情報源や、専門家など——ごとに、まとめられている。それぞれの問題の記述においては、もしその人があなたの役に立てなかったらどうすべきかについての提案を見つけられることもあるだろう。

遭遇するあらゆるデータセットを、これらの問題のすべてについて吟味することは、とてもできない。もしそうしようとすれば、決して何も公表できなくなるだろう。しかし、出くわしやすい問題の種類に慣れておくことによって、問題のせいで間違いをしでかす前に、問題を突き止められる見込みが増えるだろう。

もしこの手引きについて質問があれば、[Chris](mailto:c@qz.com)にメールしてください。うまくいきますように!

この著作品は、[クリエイティブ・コモンズ 表示 - 非営利 4.0 国際 ライセンス](http://creativecommons.org/licenses/by-nc/4.0/)のもとに供与されています。プル・リクエストをお送りくださいね!

# 翻訳

* [中国語](http://cn.gijn.org/2016/01/10/quartz%E5%9D%8F%E6%95%B0%E6%8D%AE%E6%8C%87%E5%8D%97%E7%B2%BE%E9%80%89%EF%BC%9A%E5%A4%84%E7%90%86%E6%95%B0%E6%8D%AE%E7%9A%84%E6%AD%A3%E7%A1%AE%E6%96%B9%E5%BC%8F%E4%B8%80%E8%A7%88/) (部分訳)
* [スペイン語](http://es.schoolofdata.org/guia-quartz/)

この手引きを御自分の言語に翻訳なさりたいでしょうか? どうぞ! あなたの翻訳をここに追加するには、[Chris](mailto:c@qz.com) までメールをください。

# 索引

## あなたの情報源の側で解決すべき問題

* [値が欠けている](#values-are-missing)
* [欠損値をゼロで置き換えている](#zeros-replace-missing-values)
* [データがあるはずだと分かっているのに欠けている](#data-are-missing-you-know-should-be-there)
* [行または値が繰り返されている](#rows-or-values-are-duplicated)
* [綴りが首尾一貫していない](#spelling-is-inconsistent)
* [名前の順序が首尾一貫していない](#name-order-is-inconsistent)
* [日付の形式が首尾一貫していない](#date-formats-are-inconsistent)
* [単位が指定されていない](#units-are-not-specified)
* [カテゴリの選び方がまずい](#categories-are-badly-chosen)
* [フィールド名が曖昧だ](#field-names-are-ambiguous)
* [出所が文書化されていない](#provenance-is-not-documented)
* [疑わしい値が存在する](#suspicious-values-are-present)
* [データが粗すぎる](#data-are-too-coarse)
* [総計が、公開された集計値とは違う](#totals-differ-from-published-aggregates)
* [スプレッドシートに 65536 行ある](#spreadsheet-has-65536-rows)
* [スプレッドシートに 255 列ある](#spreadsheet-has-255-columns)
* [スプレッドシートに 1900 年、1904 年、1969 年、または 1970 年の日付がある](#spreadsheet-has-dates-in-1900-1904-1969-or-1970)
* [テキストが数値に変換された](#text-has-been-converted-to-numbers)
* [数がテキストとして格納された](#numbers-have-been-stored-as-text)

## あなたが解決すべき問題

* [文字化けしている](#text-is-garbled)
* [行末が化けている](#line-endings-are-garbled)
* [データが PDF になっている](#data-are-in-a-pdf)
* [データの粒度が細かすぎる](#data-are-too-granular)
* [人間によりデータが入力された](#data-were-entered-by-humans)
* [データが書式設定や注釈と混ぜこぜになっている](#data-are-intermingled-with-formatting-and-annotations)
* [集計値が欠損値に対して計算された](#aggregations-were-computed-on-missing-values)
* [標本がランダムでない](#sample-is-not-random)
* [誤差範囲が大きすぎる](#margin-of-error-is-too-large)
* [誤差範囲が未知である](#margin-of-error-is-unknown)
* [標本が偏っている](#sample-is-biased)
* [データが人手で編集された](#data-have-been-manually-edited)
* [インフレがデータを歪める](#inflation-skews-the-data)
* [自然／季節変動がデータを歪める](#naturalseasonal-variation-skews-the-data)
* [期間が操作されている](#timeframe-has-been-manipulated)
* [準拠枠が操作されている](#frame-of-reference-has-been-manipulated)

## 第三者の専門家に解決を手助けしてもらうべき問題

* [作成者が信用ならない](#author-is-untrustworthy)
* [収集過程が不透明だ](#collection-process-is-opaque)
* [データが非現実的な精度を示す](#data-assert-unrealistic-precision)
* [説明のつかない外れ値がある](#there-are-inexplicable-outliers)
* [根底にある差異を指数が覆い隠してしまう](#an-index-masks-underlying-variation)
* [結果が p 値ハッキングを受けている](#results-have-been-p-hacked)
* [ベンフォードの法則が成り立たない](#benfords-law-fails)
* [真実であるにしては良すぎる](#too-good-to-be-true)

## プログラマに解決を手助けしてもらうべき問題

* [データが間違ったカテゴリないし地理に集約されている](#data-are-aggregated-to-the-wrong-categories-or-geographies)
* [スキャンした文書の中にデータがある](#data-are-in-scanned-documents)

# すべての問題の詳細な一覧

## あなたの情報源の側で解決すべき問題

### <a name="values-are-missing">値が欠けている</a>

いかなるデータセット中の空白または「ヌル」値にも、それが何を意味するのか知っているのだ、と自分で確信しているのでない限りは、気をつけよ。データが年ごとのものだとしたら、その年についての値がまったく収集されなかったのか? それが調査だとしたら、回答者がその質問に答えるのを拒んだのか?

欠損値のあるデータを使って作業しているときはいつでも、自分自身に「この値の欠如が何を意味するのか、私は分かっているかな?」と問いかけるべきである。もしその答えが「いいえ」なら、情報源に尋ねるべきだ。

### <a name="zeros-replace-missing-values">欠損値をゼロで置き換えている</a>

欠損値よりまずいのは、任意の値が代わりに使われる場合だ。これは、影響を考え抜かない人間による結果、ということもあり得るし、あるいは、単にヌル値の扱い方を分かっていない自動化プロセスの結果としても起こり得る。いずれにせよ、もし一連の数字の中にゼロを見たら、これらの値が本当に `0` という数なのか、それとも、そうではなくて「無」を意味しているのか、ということを自分自身に対して尋ねるべきだ (ときとして、`-1` も、この用法で使われる)。もし確信がなければ、情報源に尋ねよ。

`0` が別の方法で表現されるかもしれない、他の非数値的な値についても、同様の注意が払われるべきである。たとえば、日付についての偽の `0` 値は、しばしば、[タイムスタンプについての Unix エポック](https://en.wikipedia.org/wiki/Unix_time#Encoding_time_as_a_number) である、`1970-01-01T00:00:00Z` (協定世界時で 1970 年 1 月 1 日 0 時 0 分 0 秒) または `1969-12-31T23:59:59Z` (協定世界時で 1969 年 12 月 31 日 23 時 59 分 59 秒) と表示される。場所についての偽の `0` は、しばしば [ヌル・アイランド](https://en.wikipedia.org/wiki/Null_Island) と呼ばれるガーナ真南の大西洋上の地点である、`0°00'00.0"N+0°00'00.0"E` と表現されるかもしれず、または単に `0°N 0°E` と表現されるかもしれない。

以下も参照のこと。

* [疑わしい値が存在する](#suspicious-values-are-present)
* [スプレッドシートに 1900 年、1904 年、1969 年、または 1970 年の日付がある](#spreadsheet-has-dates-in-1900-1904-1969-or-1970)

### <a name="data-are-missing-you-know-should-be-there">データがあるはずだと分かっているのに欠けている</a>

ときには、データが欠けていて、かつ、データセットそのものからはそれが判断できないのだが、それでもなお、そのデータが何についてのものということになっているのかを知っているために、データが欠けているということを認識できることがある。米国全体を扱うデータセットがある場合、50 州すべてが表されていることを保証するために、点検を行うことができる (それから、[準州](https://en.wikipedia.org/wiki/Territories_of_the_United_States)についても忘れてはいけない。もしそのデータセットがプエルトリコ自治領を含むなら、50 は正しい数ではない)。野球選手のデータセットを扱っているなら、自分が期待するチーム数をそのデータセットが含んでいることを確認せよ。自分が知っている数人の選手が含まれていることを検証せよ。何かが欠けているようなら、自分の直感を信じよ。そして、再び情報源と照合せよ。あなたのデータの母集団は、あなたが考えているより小さいかもしれない。

### <a name="rows-or-values-are-duplicated">行または値が繰り返されている</a>

もしデータセット内に同じ行が 2 回以上現れるなら、なぜなのかを突き止めるべきだ。時には、それは行全体とは限らない。ある選挙資金データは、元の取引と同じ一意な識別子を用いた「修正」を含む。もしそのことを知らなかったら、そのデータを使って行ったいかなる計算も、間違っている可能性がある。もし何かが一意となるはずのようであれば、それが一意であることを検証せよ。もし一意でないと分かったら、なぜなのかを情報源に尋ねよ。

### <a name="spelling-is-inconsistent">綴りが首尾一貫していない</a>

綴りは、データが人手でまとめられたかどうかを伝える、もっとも明白な手段の一つだ。人々の名前だけを見ていてはいけない。それらは、綴り間違いを検出するにはもっとも難しい場所であることがしばしばなのだ。その代わりに、都市名または州が首尾一貫していない場所を探し求めよ (`Los Angelos` は非常によくある間違いの一つである)。もしそうしたものが見つかったら、データが人手でまとめられたか人手で編集されたということについて、かなり確信を持てるし、それは常に、データに対して懐疑的になる理由なのだ。人手で編集されたことのあるデータは、もっとも誤りを含みやすい。これは、そうしたデータを使うべきではない、という意味ではない。しかし、それらの誤りを手作業で訂正する必要があるかもしれないし、あるいは別の方法で、報告の際にそれらの誤りについて説明する必要があるかもしれない。

[テキスト・クラスタリング](https://github.com/OpenRefine/OpenRefine/wiki/Clustering)用の [OpenRefine's](http://openrefine.org/) ユーティリティは、ある列の中での首尾一貫しない値同士の近い一致を示唆する (たとえば、`Los Angelos` と `Los Angeles` を適合させる) ことによって、綴り訂正過程を効率化するのを手助けすることができる。しかし、[ちゃんとしたデータの出所](#provenance-is-not-documented)を保証するためには、必ず、[自分の行った変更を文書化する](https://github.com/OpenRefine/OpenRefine/wiki/Exporters)ようにせよ。


以下も参照のこと。

* [人間によりデータが入力された](#data-were-entered-by-humans)

### <a name="name-order-is-inconsistent">名前の順序が首尾一貫していない</a>

あなたのデータの中には、中東または東アジアの名前があるだろうか? 姓が常に同じ場所にあることに自信があるだろうか? あなたのデータセットの中の誰かが、[一語からなる名前を使っている](https://en.wikipedia.org/wiki/Indonesian_names#Indonesian_naming_system)という可能性があり得るだろうか? これらは、データ作成者が常習的に間違ってしまう種類の事柄なのだ。もし、民族的に多様な名前のリスト——どのような名前リストでもよい——を使って作業をしているならば、公表するのに適した何かが、`first_name` (姓) と `last_name` (名) の列を結合することによって得られるだろう、と想定する前に、少なくとも大雑把な見直しをするべきである。

* [人間によりデータが入力された](#data-were-entered-by-humans)

### <a name="date-formats-are-inconsistent">日付の形式が首尾一貫していない</a>

どちらの日付が 9 月のものだろうか。

* `10/9/15`
* `9/10/15`

もし、1 番目のものが欧州人により書かれ、2 番目のものが米国人により書かれたのだとすれば、[これらは両方とも、9 月のものである](https://en.wikipedia.org/wiki/Date_format_by_country)。データの来歴について知らなければ、確実には分からない。データがどこから来たのかを知っておけ。そして、同じ大陸出身の人々によってすべてが作成されたことを確認せよ。

* [人間によりデータが入力された](#data-were-entered-by-humans)
* [出所が文書化されていない](#provenance-is-not-documented)

### <a name="units-are-not-specified">単位が指定されていない</a>

`weight` (重さ) も `cost` (費用) も、大きさの単位について何の情報も伝えない。米国内で作り出されたデータはポンドおよびドルの単位である、と早合点して決めつけないようにせよ。科学的データはしばしばメートル法による。外国の価格は、現地通貨で指定されているかもしれない。もしデータがその単位を明らかにしていなければ、情報源に戻って究明せよ。たとえデータがその単位を明らかにしているとしても、時とともにずれてしまった可能性のある意味に用心せよ。2010 年の 1 ドルは今日の 1 ドルではない。それに、1 [トン](https://en.wikipedia.org/wiki/Short_ton)は、1 [トン](https://en.wikipedia.org/wiki/Long_ton)でもなければ、1 [トン](https://en.wikipedia.org/wiki/Tonne)でもない。

以下も参照のこと。

* [フィールド名が曖昧だ](#field-names-are-ambiguous)
* [インフレがデータを歪める](#inflation-skews-the-data)

### <a name="categories-are-badly-chosen">カテゴリの選び方がまずい</a>

`true` (真) または `false` (偽) にしかならない、と称しているのに、現実にはそうではない、といった値に用心せよ。これは、`refused` (拒絶された) または `no answer` (無回答) も有効で意味のある値であるような調査によくあることだ。別のよくある問題は、どのような種類であれ、`other` (その他) カテゴリの使い方である。もしデータセットにおけるカテゴリが、たくさんの国と `other` (その他) だったら、それは何を意味するのか? データを集めている人が、正しい答えを知らなかった、という意味か? 公海にいたということか? 国外居住者なのか? 難民なのか?

まずいカテゴリのせいで、人為的にデータを排除してしまうこともあり得る。これは、犯罪統計で頻繁に起こる。FBI は、「強姦」という犯罪を、時の経過につれて、異なる多様な方法で定義してきた。実際、FBI は、強姦とは何かということを明らかにする、というあまりにお粗末な仕事をしてきたため、多くの犯罪学者たちは、FBI の統計をまったく使うべきではないと論じるほどである。まずい定義は、ある犯罪が、あなたの予期するのとは別のカテゴリで計上されるとか、あるいはまったく計上されなかったとかいう結果をもたらすかもしれないのだ。`race` (人種) または `ethnicity` (民族) などの、定義が恣意的になりやすい話題について作業するときは、並外れてこの問題を意識せよ。

### <a name="field-names-are-ambiguous">フィールド名が曖昧だ</a>

`residence` (居住地) とは何か? 誰かが住んでいる場所なのか、それとも納税地か? 都市なのか、それとも国なのか? データ中のフィールド名は、私たちが望むほどに具体的であることは決してないものだが、明らかに二つ以上のものを意味しかねないフィールド名に対しては、格別な懸念が当てはまるはずだ。値が何を意味することになっているのかを、たとえあなたは正確に推察するのだとしても、おそらく、その曖昧性のせいで、データを集める人は、間違った値を入力してしまったことだろう。

### <a name="provenance-is-not-documented">出所が文書化されていない</a>

データは、企業や、官庁や、非営利組織や、いかれた陰謀論者などを含む、様々な種類の個人や組織によって、作成される。データは、調査やセンサや衛星などを含む、多くの異なる方法で集められる。データは、打ち込まれるかもしれず、タップされるかもしれず、あるいは走り書きされるかもしれない。自分のデータがどこから来たのかを知ることは、その限界に対する莫大な量の洞察を与えてくれる可能性がある。

たとえば、調査データは、網羅的であることは滅多にない。センサは精度にばらつきがある。官庁は、偏りのない情報を与えるのに乗り気でないことがしばしばだ。戦線を横切るという危険性ゆえに、交戦地帯からのデータには、強い地理的な偏りがあるかもしれない。こうした状況をさらに悪化させることには、これらの異なる情報源は、しばしば直列に繋がれているのだ。政策分析者は、自分が政府から得たデータを、たびたび再分配する。医師により書かれたデータは、看護師によって打ち込まれるかもしれない。そうした連鎖におけるあらゆる段階は、間違いの入り込む機会である。自分のデータがどこから来たのかを知っておけ。

以下も参照のこと。

* [単位が指定されていない](#units-are-not-specified)

### <a name="suspicious-values-are-present">疑わしい値が存在する</a>

もし自分のデータの中で、以下の値のいずれかを見たら、よく注意してそれらの値を扱え。

数:

* [`65,535`](https://en.wikipedia.org/wiki/65535_%28number%29)
* [`255`](https://en.wikipedia.org/wiki/255_%28number%29)
* [`2,147,483,647`](https://en.wikipedia.org/wiki/2147483647_%28number%29)
* [`4,294,967,295`](https://en.wikipedia.org/wiki/4294967295)
* [`555-3485`](https://en.wikipedia.org/wiki/555_%28telephone_number%29)
* `99999` (またはその他の、9の長い連なり)
* `00000` (またはその他の、0の連なり)

日付:

* [`1970-01-01T00:00:00Z`](https://en.wikipedia.org/wiki/Unix_time#Encoding_time_as_a_number)
* [`1969-12-31T23:59:59Z`](https://en.wikipedia.org/wiki/Unix_time#Encoding_time_as_a_number)
* [`January 1st, 1900`](#spreadsheet-has-dates-in-1900-1904-1969-or-1970)
* [`January 1st, 1904`](#spreadsheet-has-dates-in-1900-1904-1969-or-1970)

位置:

* [`0°00'00.0"N+0°00'00.0"E`](https://en.wikipedia.org/wiki/Null_Island) または単純に [`0°N 0°E`](https://en.wikipedia.org/wiki/Null_Island)
* 米国の郵便番号 `12345` (ニューヨーク州スケネクタディ)
* 米国の郵便番号 `90210` (カリフォルニア州ベヴァリー・ヒルズ)

これらの数のそれぞれには、人間またはコンピュータのいずれかによる特定のエラーの気配がある。もしこれらの数を見たら、それが意味する、とあなたが考えるものを、それが実際に意味している、と確認せよ!

以下も参照のこと。

* [スプレッドシートに 65536 行ある](#spreadsheet-has-65536-rows)
* [スプレッドシートに 255 列ある](#spreadsheet-has-255-columns)
* [スプレッドシートに 1900 年、1904 年、1969 年、または 1970 年の日付がある](#spreadsheet-has-dates-in-1900-1904-1969-or-1970)

### <a name="data-are-too-coarse">データが粗すぎる</a>

州を入手したが、郡が要る。雇い主を入手したが、従業員が要る。年を教えてもらったが、月がほしい。多くの場合、自分の目的にとってはあまりに集約しすぎたデータを得るものだ。

データは、一旦ひとつに合併されたら、普通は分離できない。もし粗すぎるデータを与えられたら、より具体的な何かがほしい、と情報源に求める必要があるだろう。情報源側にはそんなものがないかもしれない。情報源側は、もしそういうものを確かに持っていても、あなたにそれを与えることができないかもしれないし、あるいは、あなたに与える気がないかもしれない。多くの連邦データセット、それも、そのデータセットによって一意に識別されるかもしれない個人 (たとえば、テキサス州西部に住んでいる、唯一のソマリア国民) のプライバシを保護するために、地方レベルではアクセスできないような、多くの連邦データセットが、存在する。あなたにできることは、尋ねることだけだ。

絶対にすべきでない一つのことは、年間の値を 12 で割って、それを「1 ヶ月あたりの平均」と呼ぶことだ。値の分布を知らなければ、その数は無意味だろう (ひょっとすると、すべての事例が 1 ヶ月の間に起きたのだったり、または一つの季節の間に起きたのだったりする。ひょっとすると、データは、線形的な傾向ではなく指数関数的な傾向に従う)。それ (訳注: 分布を知らずに割って平均と称するようなこと) は間違っている。それをしては駄目だ。

以下も参照のこと。

* [データの粒度が細かすぎる](#data-are-too-granular)
* [データが間違ったカテゴリないし地理に集約されている](#data-are-aggregated-to-the-wrong-categories-or-geographies)

### <a name="totals-differ-from-published-aggregates">総計が、公開された集計値とは違う</a>

情報公開法をめぐる長い戦いの果てに、あなたは、警察が武力行使をした事件の「完全な」リストを受け取る。あなたはそのリストを開けて、リストには 2,467 行があることに気づく。素晴らしい。今や外部に報告するときだ。そう急いではいけない。そのデータセットから何であれ公開する前に、その警察署長が自分の警察署の武力行使について公表した最終時点を見つけに行こう。あなたは、6 週間前の会見において署長が「2000 回未満」と言ったことを知るかもしれない。あるいは、署長が具体的な数値を挙げたが、その数値はあなたのデータセットとは合致しない、ということを知るかもしれない。

公表された統計と生データとの間の、こうした種類の矛盾は、手がかりの非常に優れた源になり得る。答えはしばしば単純であろう。たとえば、あなたに与えられたデータは、署長が語っていたのと同じ期間を扱っていないのかもしれない。しかし、ときには、あなたは嘘を見破るだろう。いずれにせよ、公表された数値が、あなたに与えられたデータについての総計と合致することを、確認すべきである。

### <a name="spreadsheet-has-65536-rows">スプレッドシートに 65536 行ある</a>

旧式の Excel スプレッドシートが持つことを許された行の最大数は、65,536 だった。もしその行数のデータセットを受け取ったら、あなたは、ほとんど疑いなく、切り取られたデータを与えられたのである。戻って残りの部分を求めよ。より新しいバージョンの Excel は、1,048,576 行を見越していて、それゆえ、限界にぶち当たっているデータを使って作業するという可能性は、少なくなっている。

### <a name="spreadsheet-has-255-columns">スプレッドシートに 255 列ある</a>
Apple の Numbers app は 255 列のスプレッドシートしか扱えず、このアプリは、さらなる列を有するファイルを、ユーザに警告せずに切り詰めてしまうだろう。もしちょうど 255 列のデータセットを受け取ったら、そのファイルがかつて Numbers で開かれるか変換されるかしたことがあるかどうかを尋ねよ。

### <a name="spreadsheet-has-dates-in-1900-1904-1969-or-1970">スプレッドシートに1900年、1904年、1969年、または1970年の日付がある</a>

Excel が他のすべての日付を数え始める基準になっている、Excel のデフォルトの日付は、不明な理由により、`January 1st, 1900` (1900 年 1 月 1 日) である。*ただし*、Mac で Excel を使っている*のでない限りは*、なのだが。Mac では、その日付は `January 1st, 1904` (1904 年 1 月 1 日) である。Excel 内のデータが、不正確に入力または計算されて、最終的にこれら二つの日付のうちの一方になる可能性があるような、様々な道筋が存在する。もし自分のデータの中にこれらの日付を見つけたら、多分、それは問題である。

[タイムスタンプについての Unix エポック](https://en.wikipedia.org/wiki/Unix_time#Encoding_time_as_a_number)である、`1970-01-01T00:00:00Z` (協定世界時で 1970 年 1 月 1 日 0 時 0 分 0 秒) または `1969-12-31T23:59:59Z` (協定世界時で 1969 年 12 月 31 日 23 時 59 分 59 秒) という日付を、多くのデータベースやアプリケーションは、しばしば生成するだろう。換言すれば、これは、システムが、空の値または `0` 値を日付として表示しようとする場合に起こることである。

### <a name="text-has-been-converted-to-numbers">テキストが数値に変換された</a>

すべての数字が数というわけではない。たとえば、米国国勢調査局は「連邦情報処理規格 (FIPS) 符号」を使って、米国内のあらゆる場所を識別する。これらの符号は様々な長さであり、数字で表されている。しかし、これらは数*ではない*。`037` は、ロサンゼルス郡の FIPS 符号である。これは `37` という数ではない。しかし、`37` という数字は、有効な FIPS 符号、つまり、ノース・カロライナ州の符号である。Excel やその他のスプレッドシートは、しばしば、数字は数だと想定して先頭のゼロを取り去ってしまう、という間違いを犯すだろう。そのせいで、別のファイル形式に変換しようとしたり、あるいは、別のデータセットと合併しようとしたりする場合に、あらゆる種類の問題が引き起こされる可能性がある。こうしたことが起きたデータには、それが自分に与えられる前に、用心せよ。

### <a name="numbers-have-been-stored-as-text">数がテキストとして格納された</a>

スプレッドシートで作業するとき、数は、望まない書式設定で、テキストとして格納されるかもしれない。これは、データを再利用のために使えるようにしておくためではなくて、むしろデータを提示するためにスプレッドシートが最適化されている場合に、しばしば起こる。たとえば、百万ドルを "1000000" という数で表す代わりに、セルは、文字として入力されたカンマや単位やスペースの書式設定を使って、"1,000,000" または "1 000 000" または "USD 1,000,000" という文字列を含むかもしれない。Excel は、組み込み関数を使っていくつかの単純な場合について面倒を見ることができるが、あなたは、数として認識されるのに十分なほどセルがきれいになるまで、文字を剥ぎ取るための式を使う必要が、しばしばあるだろう。良い慣行は、書式設定なしに数を格納して、列の名前またはメタデータの中に補助的情報を含めておくことである。

## あなたが解決すべき問題

### <a name="text-is-garbled">文字化けしている</a>

すべての文字は、コンピュータにより、数として表現されている。符号化の問題は、数の特定の集合によってテキストが表現され (「符号化」と呼ばれる)、かつ、あなたがその符号化が何なのかを知らない場合に生じる問題だ。これは、データ中のテキストがごみのように、あるいはこんなふう ��� に見えるという、[文字化け](https://en.wikipedia.org/wiki/Mojibake)と呼ばれる現象につながる。

大多数の場合、あなたのテキストエディタまたはスプレッドシート・アプリケーションが正しい符号化を理解してくれるだろうが、もしテキストエディタまたはスプレッドシート・アプリケーションのせいで滅茶苦茶になったら、あなたは、途中に奇妙な文字のある状態で誰かの名前を公開しかねない。あなたの情報源は、データがどの符号化によるものなのかを、あなたに教えることができるはずである。教えられない場合にも、概ね、かなり信頼できるような、いくつかの推測法がある。

### <a name="line-endings-are-garbled">行末が化けている</a>

すべてのテキストと、CSV のような「テキストデータ」ファイルは、行の終わりを表すための不可視の文字を使っている。Windows コンピュータと Mac コンピュータと Linux コンピュータは、これらの行末文字が何であるべきか、ということについて、歴史的に、意見が異なってきた。ある一つのオペレーティング・システム上で保存されたファイルを、別のオペレーティング・システムから開こうとすると、ときとして、Excel または他のアプリケーションが適切に改行を見分けられなくなってしまう。

典型的には、これは、単にファイルを任意の汎用テキストエディタで開いて再保存することで、容易に解決する。もしファイルが例外的なほど大きければ、コマンドライン・ツールを使うか、プログラマの助けを求めることを、考える必要があるかもしれない。この問題については、[ここ](https://nicercode.github.io/blog/2013-04-30-excel-and-line-endings/)で、もっと読むことができる。

### <a name="data-are-in-a-pdf">データが PDF になっている</a>

厖大な量のデータ——とりわけ政府のデータ——は、PDF 形式でのみ利用可能である。もし、PDF の内部に、実際のテキストのデータがあるなら、それらのデータを抽出するための良い選択肢がいくつかある ([スキャンした文書](#data-are-in-scanned-documents)を入手した場合は、別の問題である)。一つの優れた無料のツールは、[Tabula](http://tabula.technology/) である。しかし、もしあなたが Adobe Creative Cloud を持っているなら、Acrobat Pro の利用権もあり、Acrobat Pro には PDF の中の表を Excel にエクスポートするための優れた機能があるのだ。どちらの解決策でも、ほとんどの表形式データを PDF から抽出することができるはずだ。

以下も参照のこと。

* [スキャンした文書の中にデータがある](#data-are-in-scanned-documents)

### <a name="data-are-too-granular">データの粒度が細かすぎる</a>

これは、[データが粗すぎる](#data-are-too-coarse) ということの逆である。この場合、あなたは、郡を入手したが、州がほしい。あるいは、月を入手したが、年がほしいというわけだ。幸い、これは普通、とても簡単だ。

Excel もしくは Google Docs のピボットテーブル機能を使うことによって、または SQL データベースを使うことによって、または特別あつらえのコードを書くことによって、データを集約できる。ピボットテーブルは、すべての報告者が学ぶべき素晴らしいツールだが、確かに限界もある。例外的なほど大きなデータベース、または稀なグループへの集約については、プログラマに尋ねるべきであり、検証・再利用するのがより容易な解決法を、プログラマは巧みに作ることができる。

以下も参照のこと。

* [データが粗すぎる](#data-are-too-coarse)
* [データが間違ったカテゴリないし地理に集約されている](#data-are-aggregated-to-the-wrong-categories-or-geographies).

### <a name="data-were-entered-by-humans">人間によりデータが入力された</a>

人間によるデータ入力は、とてもよくある問題なので、その兆候が、本文書で記述されている他の論点のうちの少なくとも 10 箇所において言及されているほどである。データを滅茶苦茶にするには、妥当性の検証なしに一人の人間にデータを打ち込ませておくより悪い方法はない (訳注: うまく訳せていないかも)。たとえば、筆者はかつてイリノイ州クック郡の、完全な犬の鑑札データベースを手に入れた。自分の犬を登録する人に対して、リストから品種を選ぶことを求める代わりに、そのシステムの作成者は、単純に、打ち込むためのテキストフィールドを与えたのだ。その結果、このデータベースは、`Chihuahua` (チワワ) の少なくとも 250 通りの綴り方を含んでいた。利用可能な最善のツールを使ったとしても、こういう汚いデータは救いようがない。それらのツールは実質的に無意味である。これは、犬のデータについてはそれほど重要ではないが、負傷兵あるいは株式相場速報表示機について起こってほしいとは思わないだろう。人間が入力したデータに注意せよ。


### <a name="data-are-intermingled-with-formatting-and-annotations">データが書式設定や注釈と混ぜこぜになっている</a>

HTML や XML などの、データの複雑な表現は、データと書式をすっきり分離することを想定しているが、これは、スプレッドシートのような普通の表形式の表現には該当しない。それでも、人々はまだ試している。スプレッドシートとして提供されるデータについてのよくある問題は、データのうちの最初の何行かが、列見出しまたはデータ自体ではなく、実際にはむしろ、説明だったり、あるいはデータについての注釈だったりするだろう、ということである。キーまたはデータ辞書も、スプレッドシートの中途に配置されることがある。見出し行が繰り返されているかもしれない。あるいは、スプレッドシートは、別々のシートに分けて、ではなく、むしろ同一シート内に、次から次へと、複数の表を含むだろう (そしてそれらの表は、異なる列見出しを有するかもしれない)。

これらの場合のすべてにおいて、主たる解決策は、単に問題を突き止めることである。こうした種類の問題があるスプレッドシート上で、何らかの分析を実行しようとすることは、明らかに失敗するであろうし、ときには非自明な理由で失敗するだろう。新しいデータを初めて見る場合には、データの合間に挿入された、余計な見出し行または他の書式設定文字が存在しないことを確認しておく、ということは、常に良い考えである。

### <a name="aggregations-were-computed-on-missing-values">集計値が欠損値に対して計算された</a>

100 行あって、かつ、`cost` (費用) と呼ばれる列があるようなデータセットを想像せよ。その行のうちの 50 行で、`cost` 列は空白である。その列の平均値とは何だろう? `sum_of_cost / 50` なのか? それとも `sum_of_cost / 100` なのか? 一つの決定的な答えというものはない。一般には、データを欠いている列について集計値を計算しようとしている場合、欠損行をまず取り除くことによって、安全にその計算を行うことができる。しかし、異なる行が値を欠いているような二つの異なる列からの集計値同士を、比較しないように注意せよ! 場合によっては、欠損値が正当にも `0` と解釈されることがあるかもしれない。もし確信がなければ、専門家に尋ねるか、あるいは、単にそういうことはしないように。

これは、あなたが自分の分析においてしでかす可能性のある間違いだが、他人がしでかして、そしてあなたに回してくる可能性のある間違いでもある。そのため、計算済みの集計値とともにデータがあなたのところへやってきたら、この間違いに用心せよ。

以下も参照のこと。

* [値が欠けている](#values-are-missing)
* [欠損値をゼロで置き換えている](#zeros-replace-missing-values)

### <a name="sample-is-not-random">標本がランダムでない</a>

ランダムでない標本抽出の誤りは、調査または他の標本抽出されたデータセットが、意図的に、または偶然に、母集団全体をカバーし損ねるときに、生じる。この誤りは、時刻から回答者の母語にまで及ぶ様々な理由で起こり得るし、社会学的研究における誤りの、ありふれた発生源でもある。この誤りは、研究者が、自分たちは完全なデータセットを持っていると考えて、その一部のみを使って作業することを選ぶ場合などに、それほど明白でない理由によって起こることもあり得る。もし、何らかの理由により元のデータセットが不完全であれば、そのデータセットの標本から引き出されたいかなる結論も、正しくないだろう。ランダムでない標本を修正するためにあなたができる唯一のことは、そのデータを使うのを回避することである。

以下も参照のこと。

* [標本が偏っている](#sample-is-biased)

### <a name="margin-of-error-is-too-large">誤差範囲が大きすぎる</a>

非常に大きな誤差範囲をともなう数値の無分別な使用よりも多くの報告エラーを引き起こすような、単一の他の論点を、筆者は知らない。誤差範囲 (MOE) は通常、調査データに関連づけられている。報告者が MOE に遭遇する、もっともありがちな場所は、世論調査データまたは米国国勢調査局の [American Community Survey](https://www.census.gov/programs-surveys/acs/) データを使う場合である。MOE は、あり得る真の値の範囲の大きさである。MOE は、数値として (`400 +/- 80` などと) 表されるかもしれないし、あるいは、全体のうちの割合として (`400 +/- 20%` などと) 表されるかもしれない。当該の母集団が小さいほど、MOE は大きくなるだろう。たとえば、2014 年の 5 年毎の ACS 推計値によれば、ニューヨークに住んでいるアジア系の人数は `1,106,989 +/- 3,526` (0.3%) である。フィリピン人の人数は `71,969 +/- 3,088` (4.3%) である。サモア人の人数は `203 +/- 144` (71%) である。

はじめの二つの数値は、報告しても安全だ。三番目の数値は、公表される報道では決して使われるべきではない。どういうときに数値は、使うのに十分なほどの精度がないのか、ということについての、一つの規則というものはない。しかし、大雑把な目安としては、MOE が 10% を超えている、いかなる数値を使うことについても、慎重であるべきだ。

以下も参照のこと。

* [誤差範囲が未知である](#margin-of-error-is-unknown)

### <a name="margin-of-error-is-unknown">誤差範囲が未知である</a>

ときには、問題は、誤差範囲が[大きすぎる](#margin-of-error-is-too-large)ことではなくて、そもそも誤差範囲がどれくらいだったのか、誰も今までわざわざ計算しなかったことだったりする。これは、科学的でない意識調査にともなう一つの問題である。MOE (誤差範囲) を計算しなければ、結果がどれほど高精度なのかを知ることは不可能である。一般的な規則としては、調査から得られたデータを持っている場合はいつでも、あなたは、MOE がいくつなのかを尋ねるべきである。もし情報源側が教えられないなら、そうしたデータは、おそらく、どのような本格的分析のためにも使う価値はない。

以下も参照のこと。

* [誤差範囲が大きすぎる](#margin-of-error-is-too-large)

### <a name="sample-is-biased">標本が偏っている</a>

[ランダムでない標本](#sample-is-not-random)のように、偏った標本は、標本抽出がどのように実行されるのかについての注意の欠如に起因する。あるいは、標本抽出を故意に捻じ曲げることに起因する。標本抽出がインターネット上で行われ、かつ、裕福な人々ほど頻繁には、より貧しい人々はインターネットを使わないという理由から、標本が偏っているかもしれない。どの母集団のうちであれ、結果を歪めかねないような比例部分を、調査がカバーしている、ということを確実にするために、調査を注意深く重み付けせねばならない (訳注: うまく訳せていないかも)。これを完璧にこなすのはほとんど不可能であり、ゆえに、しばしば間違って行われてしまうのである。

以下も参照のこと。

* [標本がランダムでない](#sample-is-not-random)

### <a name="data-have-been-manually-edited">データが人手で編集された</a>

人手による編集は、事後に生じるという点を除いて、[人間によりデータが入力された](#data-were-entered-by-humans)というのとほとんど同じ問題である。実際、もともと人間によって入力されたデータを修正するための試みにおいて、データはしばしば人手で編集される。編集を行っている人物が、元のデータについての完全な知識をそなえていない場合に、問題が忍び寄り始める。筆者はかつて、ある人が無意識に、データセット中の名前を `Smit` から `Smith` へと「訂正」するのを見た。その人の名前は本当に `Smith` だったのか? 筆者は知らないが、その値が今や問題であることを確かに知っている。その変更の記録がなければ、それが何であるべきかを検証することは不可能である。

人手による編集についての問題は、なぜ常に、[きちんと文書化された出所](#provenance-is-not-documented)があなたのデータにあることを保証したいのか、ということの、一つの理由である。出所の欠如は、誰かがそれをいじったかもしれないということの、有益な徴 (しるし) である。研究者や政策分析者は、しばしば政府からデータを得て、それをいじり、それから、ジャーナリストに再配布する。研究者や政策分析者による変更の記録が何もなければ、かれらが行った変更が正当化されるかどうかを知ることは不可能だ。実現可能なときはいつでも常に、*一次資料*か、少なくともあなたに可能なうちで最も初期の版を手に入れるよう試み、そして、その手に入れたものから、自分自身の分析を行え。

以下も参照のこと。

* [出所が文書化されていない](#provenance-is-not-documented)
* [人間によりデータが入力された](#data-were-entered-by-humans)

### <a name="inflation-skews-the-data">インフレがデータを歪める</a>

通貨インフレは、金銭は時とともに価値が変わる、ということを意味する。数値が「インフレ調整済み」なのかどうかを、単にそれらの数値を見るだけで判断できるような方法は、存在しない。もしあなたがデータを入手して、そのデータが調整済みかどうか確信がなければ、情報源に確認せよ。もしデータが調整済みでなければ、あなたは調整を行いたいと思う可能性が高いだろう。この[インフレ調整器](http://inflation-adjust.herokuapp.com/)は、手始めに良いところだ。

以下も参照のこと。

* [自然／季節変動がデータを歪める](#nationalseasonal-variation-skews-the-data)

### <a name="naturalseasonal-variation-skews-the-data">自然／季節変動がデータを歪める</a>

多くの種類のデータは、根底にある何らかの力のために、自然に変動する。このことの最もよく知られた例は、季節とともに変動する雇用率である。この変動を補償する多様な方法を、経済学者たちは開発してきた。それらの方法の詳細は格別に重要なわけではないが、いま自分が使っているデータが「季節調整済み」かどうかを知っていることは重要である。もしデータが季節調整済みでなくて、かつ、雇用率を月ごとに比べたいなら、あなたは、おそらく、調整されたデータを情報源から入手したいと思うだろう (自分でデータを調整するのは、インフレの場合よりずっと難しい)。

以下も参照のこと。

* [インフレがデータを歪める](#inflation-skews-the-data)

### <a name="timeframe-has-been-manipulated">期間が操作されている</a>

特定の時点で終了または開始するデータをあなたに与えることによって、情報源は、うっかり、または意図的に、世界を歪め得る。強い影響力のある例については、2015 年の、広範囲に報告された「全国的な突然の犯罪の増加」を参照のこと。突然の犯罪の増加などなかった。あったのは、単に最近の 2、3 年と比べた場合の、特定の都市での、一連の急増だった。もしジャーナリストが、もっと広範囲の期間を吟味していたなら、米国中のほとんどあらゆるところで 10 年前には暴力犯罪がもっと多かったことが分かっただろう。それに、20 年前には、暴力犯罪はほとんど 2 倍あった。

限られた期間を扱うデータを持っている場合は、あなたがデータを持っているまさに最初の期間から計算を始めるのを避けるように努めよ。データに入って 2、3 年 (または 2、3 ヶ月 または 2、3 日) 後から始めれば、あなたは、単一の追加的データポイントがあることにより無効化されるであろう比較をしているのではない、と自信を持てる (訳注: ここは訳に自信がない)。

以下も参照のこと。

* [準拠枠が操作されている](#frame-of-reference-has-been-manipulated)

### <a name="frame-of-reference-has-been-manipulated">準拠枠が操作されている</a>

犯罪統計はしばしば、政治的意図のために、犯罪がとても多かった年と比較することで操作される。これは、変化 (2004 年から `60%` 減) として表現され得るし、あるいは、指数 (`40` ただし 2004 年を 100 とする) によっても表現され得る。これらのいずれの場合であれ、2004 年は、比較するのに適切かもしれないし、適切でないかもしれない。2004 年は、並外れて犯罪の多い年だったかもしれないのだ。

こういうことは、場所同士を比較するときも生じる。もしある一つの国を悪く見せたければ、単純に、その国についてのデータを、最善を尽くしているどのような国とであれ、比べて表せばよい。

この問題は、強い確証バイアス (「私が考えた、まさにその通りに、犯罪が増えている!」) を人々が持っている議題において、思いがけず生じる傾向がある。可能であるときはいつでも、数がどう移り変わるかを見るために、いくつかの異なる開始点からの割合同士を試しに比較してみよ。そして、あなたが何をするのであれ、自分が重要だと考える論点を主張するために*この技法を自分で使ってはならない*。

以下も参照のこと。

* [期間が操作されている](#timeframe-has-been-manipulated)

## 第三者の専門家に解決を手助けしてもらうべき問題

### <a name="author-is-untrustworthy">作成者が信用ならない</a>

ときには、あなたが持っている唯一のデータが、どちらかというとむしろ信頼したくない情報源からのものだったりする。いくつかの状況では、それで十分だ。何挺の銃が製造されるのかを知っている人々は、ただ銃器製造業者のみである。しかし、もし疑わしい作成者からのデータを持っているなら、常にそれを、別の専門家と一緒に検査せよ。さらに良いのは、そのデータを、二人か三人と一緒に検査することだ。実質的に裏付けとなる証拠を持っているのでない限り、偏った情報源からのデータを公表してはいけない。

### <a name="collection-process-is-opaque">収集過程が不透明だ</a>

事実に反する想定や、誤りや、あるいは全くの虚偽が、これらのデータ収集過程に持ち込まれるのは、とてもよくあることだ。この理由から、使われている手法に透明性があることが重要なのである。データセットがどのようにして集められたのかを、あなたが正確に知っているだろう、ということは滅多にない。しかし、問題の兆候には、[非現実的な精度を示す](#data-asserts-unrealistic-precision)数や、[真実であるにしては良すぎる](#too-good-to-be-true)データが含まれ得る。

ときには、起源の説明がまさに胡散くさい、ということもある。かくかくしかじかの研究者が、シカゴの南側から来た 50 人の活動的な暴力団構成員と、本当に面談したのか? もし、データが集められた方法が疑わしく、かつ、情報源があなたに対して[鉄壁の証拠](#provenance-is-not-documented)を提供できないのであれば、あなたは常に他の専門家と一緒になって、データは当然ながら説明された方法で収集されたのだろう、ということを検証すべきである。

以下も参照のこと。

* [出所が文書化されていない](#provenance-is-not-documented)
* [データが非現実的な精度を示す](#data-assert-unrealistic-precision)
* [真実であるにしては良すぎる](#too-good-to-be-true)

### <a name="data-assert-unrealistic-precision">データが非現実的な精度を示す</a>

自然科学の外側では、わずかなものしか、日常的に小数点以下 3 桁以上の精度で計測されたりしない。工場の排出量を小数点以下 7 桁まで示す、と称するデータセットが舞い込んで来たら、それは、そのデータセットが他の値から推計されたという動かぬ証拠である。それ自体は問題ではないかもしれないが、推計値について透明性が高いことは重要である。推計値はしばしば間違っている。

### <a name="there-are-inexplicable-outliers">説明のつかない外れ値がある</a>

筆者は最近、インターネット中 (じゅう) の異なる宛先にメッセージが届くのにどれくらい時間がかかるのか、についてのデータセットを作成した。それらの時間のすべては、三つを除いて、`0.05` 秒から `0.8` 秒までの範囲内であった。残りの三つはどれも `5,000` 秒を超えていた。これは、データの生成の際に何かがうまくいかなかったという、重大な危険信号である。この特定の事例では、筆者が書いたコード中のエラーが、すべての他のメッセージが送受信されている間は数え続けるという、ある種の不具合を引き起こしていた。

このような外れ値は、あなたの統計を劇的に台無しにし得る——とりわけ、平均値を使っている場合には (おそらく、中央値を使っていてしかるべきなのだ)。新しいデータセットがあるときにはいつも、最大値と最小値を見て、それらが合理的な範囲内にあることを保証する、というのが良い考えである。データが自己の正当性を示す場合、あなたは、[標準偏差](https://en.wikipedia.org/wiki/Standard_deviation)または[中央絶対偏差](https://en.wikipedia.org/wiki/Median_absolute_deviation)を用いて、より統計学的に厳格な分析も行いたいかもしれない。

この作業を行うことの余恵として、外れ値はしばしば、作り話の手がかりを見つけるための素晴らしい道筋となる。もし本当に、インターネットを介してメッセージを送るのに 5,000 倍も長い時間がかかるような一つの国があったというなら、それはものすごい作り話だろう。

### <a name="an-index-masks-underlying-variation">根底にある差異を指数が覆い隠してしまう</a>

論点の流行に乗りたい分析者は、しばしば、成り行きを追いかけるために、様々な値の指数を作る。指数を使うことについて、本質的に間違っていること、というものはない。指数は、素晴らしい説明力を持ち得る。しかし、異質な尺度同士を組み合わせる指数に関しては、慎重であることが重要だ。

たとえば、国連[ジェンダー不平等指数 (GII)](http://hdr.undp.org/en/content/gender-inequality-index-gii) は、平等に向かっての女性の前進に関連する、いくつかの尺度を組み合わせている。GII で使われている尺度のうちの一つは、「議会における女性の代表」である。世界の中で二つの国には、その国の議会において性別の代表を義務づける法律がある。中国とパキスタンである。その結果、これら二つの国は、すべての他の面では似たような国々よりも、指数では遥かに良い成績を挙げている。これは適正だろうか? そのことは、本当に重要な訳ではない。なぜなら、この因子について知らない誰にとっても、これは困惑を引き起こすものだからである。GII や類似の指数は、その根底にある変数が当該指数を予期せぬやり方で揺り動かしたりしないことを保証するために、常に、注意深い分析とともに使われるべきである。

### <a name="results-have-been-p-hacked">結果が p 値ハッキングを受けている</a>

p 値ハッキングとは、統計的に有意な知見を得るために、意図的にデータを改変したり、統計的分析を変更したり、あるいは、選択的に結果を報告することである。これの例は、以下のものを含む。一旦、有意な結果を得たところで、データを収集するのをやめること。有意な結果を得るために観察結果を取り除くこと。あるいは、何回も分析を行って、有意な少数の分析だけを報告すること。この問題については、いくつかの[優れた報告](http://fivethirtyeight.com/features/science-isnt-broken)がある。

もしあなたが、p 値とは何であり何を意味するのかを、あなたが理解しておく必要があるような研究の成果を公開するつもりなら、その成果には使うだけの価値があるのかどうかについて、しかるべき助言を受けた決定を下すようにせよ。ジャーナリストが p 値を理解していないがゆえに、とてもたくさんのごみのような研究結果が、主要な刊行物になっているのだ。

以下も参照のこと。

* [誤差範囲が大きすぎる](#margin-of-error-is-too-large)

### <a name="benfords-law-fails">ベンフォードの法則が成り立たない</a>

[ベンフォードの法則](https://en.wikipedia.org/wiki/Benford's_law)とは、小さな数字 (1, 2, 3) の方が大きな数字 (7, 8, 9) よりもずっと頻繁に数値の先頭に現れる、と述べる学説である。ベンフォードの法則は、間違ったふうに適用されることが実際にはよくあるのだが、理論的には、会計実務または選挙結果における異常を検出するのに使うことができる。人を騙すためにデータセットが作成または改変されたのではないか、とあなたが疑っている場合には、ベンフォードの法則は、優れた最初の検査ではある。しかし、データが改竄されていたと結論を出す前に、必ず、専門家と一緒に、結果を検証すべきである。

### <a name="too-good-to-be-true">真実であるにしては良すぎる</a>

世論についてのグローバルなデータセットなどない。シベリアに住んでいる人の正確な数は、誰も知らない。犯罪統計は境界を超えて比較することができない。米国政府は、手元にどれだけの核分裂性物質を保管しているのかを、あなたに知らせたりしないだろう。

あなたがどうしても知りようのない何かを表しているのだ、と称する、いかなるデータも警戒せよ。それはデータではない。それは誰かの推計値であり、おそらく間違っている。はたまた……それはでっち上げということもあり得るので、それを調べてくれるよう、専門家に依頼せよ。

## プログラマに解決を手助けしてもらうべき問題

### <a name="data-are-aggregated-to-the-wrong-categories-or-geographies">データが間違ったカテゴリないし地理に集約されている</a>

自分のデータが、ほぼ適切な詳細度である ([粗すぎる](#data-are-too-coarse)訳でもなく、[粒度が細かすぎる](#data-are-too-granular)訳でもない) のだが、自分が望むのとは違うグループ分けで集約されていた、ということが、ときどきある。これの古典的な例は、自分は都市の地区による集約の方を望んでいるのに、郵便番号による集約がなされているようなデータである。多くの場合、これは、より粒度の細かいデータを情報源から得ないと、解決できない問題である。しかし、ときには、一つのグループから別のグループへと、データを相応にマッピングすることができる。これは、工程内に持ち込まれるかもしれない[誤差範囲](#margin-of-error-is-too-large)についての注意深い理解があるときにしか、引き受けてはならない。間違ったグループへと集約されているデータを入手したら、集約し直すことができるかどうかをプログラマに尋ねよ。

以下も参照のこと。

* [データが粗すぎる](#data-are-too-coarse)
* [データの粒度が細かすぎる](#data-are-too-granular)
* [誤差範囲が大きすぎる](#margin-of-error-is-too-large)

### <a name="data-are-in-scanned-documents">スキャンした文書の中にデータがある</a>

情報公開法のおかげで、あなたにデータを与えるように——たとえ本当は与えたくなくても——政府が要求される、ということは頻繁にある。これらの場合にとてもよくある作戦は、当局にとっては、ページをスキャンしたものまたはページの写真をあなたに与えることである。これらは、実際の画像ファイルのこともある。あるいは、もっとありそうなことだが、PDF にまとめ上げられていることだろう。

テキストを画像から抽出してデータへと戻すことは、可能である。これは、光学式文字認識 (OCR) と呼ばれるプロセスを介して、行われる。近代的な OCR は、ほとんど 100% の精度を出せることがしばしばだが、文書の性質によりけり、という面が非常に大きい。データを抽出するために OCR を使うときはいつでも、その結果が原文と一致することを確認するための工程が欲しいだろう。

OCR のために文書をアップロードできるウェブサイトが多数あるが、あなたの特定の文書のためにプログラマが調整できるかもしれない無料のツールもある。あなたが持っている特定の文書にとっては何が最良の戦略なのかを、プログラマに尋ねよ。

以下も参照のこと。

* [データが PDF になっている](#data-are-in-a-pdf)
